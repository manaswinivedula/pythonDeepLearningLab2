{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1,2&3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manaswinivedula/pythonDeepLearningLab2/blob/master/1%2C2%263.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xitplqMNk_Hc",
        "outputId": "ed4f60d2-878d-4056-c438-352dac39a112",
        "colab": {
          "height": 420
        }
      },
      "source": [
        "#@title Introducing Colaboratory { display-mode: \"form\" }\n",
        "#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('inN8seMm7UI', width=600, height=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"400\"\n",
              "            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "5626194c-e802-4293-942d-2908885c3c1f",
        "colab": {
          "height": 35
        }
      },
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\".\n",
        "\n",
        "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gE-Ez1qtyIA",
        "outputId": "8d2e4259-4682-4e19-b683-7b9087f28820",
        "colab": {
          "height": 35
        }
      },
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lSrWNr3MuFUS"
      },
      "source": [
        "For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Rh3-Vt9Nev9"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n",
        "\n",
        "### Working with Notebooks in Colaboratory\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "- <img src=\"/img/new.png\" height=\"20px\" align=\"left\" hspace=\"4px\" alt=\"New\"></img>\n",
        " [TensorFlow 2 in Colab](/notebooks/tensorflow_version.ipynb)\n",
        "\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
        "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
        "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
        "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
        "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
        "\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "## Machine Learning Examples: Seedbank\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
        "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
        "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
        "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
        "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKbQAWC11AdI",
        "colab_type": "code",
        "outputId": "bb8113db-3f33-45c3-ba8a-745a15620b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorboardcolab import *\n",
        "from __future__ import print_function\n",
        "import os\n",
        "from datetime import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import metrics\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import plot_model\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tbc=TensorBoardColab()\n",
        "df = pd.read_csv('Boston.csv')\n",
        "kc_data = pd.DataFrame(df, columns=[\"crim\",\"zn\",\"indus\",\"chas\",\"nox\",\"rm\",\"age\",\"dis\",\"rad\",\"tax\",\"ptratio\",\"lstat\",\"medv\"])\n",
        "label_col = 'medv'\n",
        "#print(kc_data.describe())\n",
        "kc_x_train, kc_x_valid, kc_y_train, kc_y_valid = train_test_split(kc_data.iloc[:,0:12], kc_data.iloc[:,12],\n",
        "                                                    test_size=0.3, random_state=87)\n",
        "\n",
        "np.random.seed(155)\n",
        "def norm_stats(df1, df2):\n",
        "    dfs = df1.append(df2)\n",
        "    minimum = np.min(dfs)\n",
        "    maximum = np.max(dfs)\n",
        "    mu = np.mean(dfs)\n",
        "    sigma = np.std(dfs)\n",
        "    return (minimum, maximum, mu, sigma)\n",
        "\n",
        "def z_score(col, stats):\n",
        "    m, M, mu, s = stats\n",
        "    df2 = pd.DataFrame()\n",
        "    for c in col.columns:\n",
        "        df2[c] = (col[c]-mu[c])/s[c]\n",
        "    return df2\n",
        "\n",
        "stats = norm_stats(kc_x_train, kc_x_valid)\n",
        "arr_x_train = np.array(z_score(kc_x_train, stats))\n",
        "arr_y_train = np.array(kc_y_train)\n",
        "arr_x_valid = np.array(z_score(kc_x_valid, stats))\n",
        "arr_y_valid = np.array(kc_y_valid)\n",
        "print('Training shape:', arr_x_train.shape)\n",
        "print('ddd',arr_y_train.shape)\n",
        "print('Training samples: ', arr_x_train.shape[0])\n",
        "print('Validation samples: ', arr_x_valid.shape[0])\n",
        "\n",
        "#basic_model_1 created model with some parameters\n",
        "def basic_model_1(x_size, y_size):\n",
        "    t_model = Sequential()\n",
        "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n",
        "    t_model.add(Dense(50, activation=\"relu\"))\n",
        "    t_model.add(Dense(y_size))\n",
        "    t_model.compile(loss='mean_squared_error',\n",
        "        optimizer=Adam(),\n",
        "        metrics=[metrics.mae])\n",
        "    return(t_model)\n",
        "\n",
        "#basic_model_2 is different from basic_model_1 but doing the same task with different structure\n",
        "def basic_model_2(x_size, y_size):\n",
        "    t_model = Sequential()\n",
        "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n",
        "    t_model.add(Dropout(0.1))\n",
        "    t_model.add(Dense(50, activation=\"relu\"))\n",
        "    t_model.add(Dense(20, activation=\"relu\"))\n",
        "    t_model.add(Dense(y_size))\n",
        "    keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "    t_model.compile(loss='mean_squared_error',\n",
        "        optimizer=Adam(),\n",
        "        metrics=[metrics.mae])\n",
        "    tensorboard = TensorBoard(log_dir=\"logs/final1\",histogram_freq=0, write_graph=True, write_images=True)\n",
        "    return(t_model)\n",
        "\n",
        "model = basic_model_2(arr_x_train.shape[1], 1)\n",
        "\n",
        "model.summary()\n",
        "epochs = 20\n",
        "batch_size =32\n",
        "history = model.fit(arr_x_train, arr_y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    shuffle=True,\n",
        "    verbose=2, # Change it to 2, if wished to observe execution\n",
        "    validation_data=(arr_x_valid, arr_y_valid),callbacks=[TensorBoardColabCallback(tbc)])\n",
        "\n",
        "train_score = model.evaluate(arr_x_train, arr_y_train, verbose=0)\n",
        "valid_score = model.evaluate(arr_x_valid, arr_y_valid, verbose=0)\n",
        "\n",
        "print('Train MAE: ', round(train_score[1], 4), ', Train Loss: ', round(train_score[0], 4))\n",
        "print('Val MAE: ', round(valid_score[1], 4), ', Val Loss: ', round(valid_score[0], 4))\n",
        "\n",
        "\n",
        "keras_callbacks = [\n",
        "    ModelCheckpoint('/tmp/keras_checkpoints/model.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=True, verbose=2),\n",
        "    ModelCheckpoint('/tmp/keras_checkpoints/model.{epoch:02d}.hdf5', monitor='val_loss', save_best_only=True, verbose=0),\n",
        "    TensorBoard(log_dir='./model_3', histogram_freq=0, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None),\n",
        "    EarlyStopping(monitor='val_mean_absolute_error', patience=20, verbose=0)\n",
        "]\n",
        "\n",
        "def plot_hist(h, xsize=6, ysize=10):\n",
        "    # Prepare plotting\n",
        "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "    plt.rcParams[\"figure.figsize\"] = [xsize, ysize]\n",
        "    fig, axes = plt.subplots(nrows=4, ncols=4, sharex=True)\n",
        "\n",
        "    # summarize history for MAE\n",
        "    plt.subplot(211)\n",
        "    plt.plot(h['mean_absolute_error'])\n",
        "    plt.plot(h['val_mean_absolute_error'])\n",
        "    plt.title('Training vs Validation MAE')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.subplot(212)\n",
        "    plt.plot(h['loss'])\n",
        "    plt.plot(h['val_loss'])\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot it all in IPython (non-interactive)\n",
        "    plt.draw()\n",
        "    plt.show()\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://a473919c.ngrok.io\n",
            "Training shape: (354, 12)\n",
            "ddd (354,)\n",
            "Training samples:  354\n",
            "Validation samples:  152\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_17 (Dense)             (None, 100)               1300      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 20)                1020      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 7,391\n",
            "Trainable params: 7,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 354 samples, validate on 152 samples\n",
            "Epoch 1/20\n",
            " - 0s - loss: 564.8102 - mean_absolute_error: 22.1616 - val_loss: 605.9079 - val_mean_absolute_error: 22.6302\n",
            "Epoch 2/20\n",
            " - 0s - loss: 525.9120 - mean_absolute_error: 21.3147 - val_loss: 561.9671 - val_mean_absolute_error: 21.7421\n",
            "Epoch 3/20\n",
            " - 0s - loss: 473.7074 - mean_absolute_error: 20.1483 - val_loss: 489.5677 - val_mean_absolute_error: 20.1944\n",
            "Epoch 4/20\n",
            " - 0s - loss: 384.4545 - mean_absolute_error: 17.9528 - val_loss: 373.5006 - val_mean_absolute_error: 17.3762\n",
            "Epoch 5/20\n",
            " - 0s - loss: 268.3255 - mean_absolute_error: 14.3530 - val_loss: 230.9817 - val_mean_absolute_error: 12.6941\n",
            "Epoch 6/20\n",
            " - 0s - loss: 163.0613 - mean_absolute_error: 9.8816 - val_loss: 139.7311 - val_mean_absolute_error: 9.3390\n",
            "Epoch 7/20\n",
            " - 0s - loss: 115.4517 - mean_absolute_error: 8.2551 - val_loss: 107.6513 - val_mean_absolute_error: 7.9871\n",
            "Epoch 8/20\n",
            " - 0s - loss: 90.3262 - mean_absolute_error: 7.2652 - val_loss: 90.5645 - val_mean_absolute_error: 7.2751\n",
            "Epoch 9/20\n",
            " - 0s - loss: 74.5143 - mean_absolute_error: 6.5632 - val_loss: 75.2814 - val_mean_absolute_error: 6.6275\n",
            "Epoch 10/20\n",
            " - 0s - loss: 59.5502 - mean_absolute_error: 5.9051 - val_loss: 60.9560 - val_mean_absolute_error: 5.9457\n",
            "Epoch 11/20\n",
            " - 0s - loss: 50.3722 - mean_absolute_error: 5.3191 - val_loss: 51.7610 - val_mean_absolute_error: 5.3604\n",
            "Epoch 12/20\n",
            " - 0s - loss: 42.4836 - mean_absolute_error: 4.8501 - val_loss: 46.6609 - val_mean_absolute_error: 4.9720\n",
            "Epoch 13/20\n",
            " - 0s - loss: 37.7821 - mean_absolute_error: 4.5519 - val_loss: 42.7657 - val_mean_absolute_error: 4.7580\n",
            "Epoch 14/20\n",
            " - 0s - loss: 35.6844 - mean_absolute_error: 4.4167 - val_loss: 39.8345 - val_mean_absolute_error: 4.6048\n",
            "Epoch 15/20\n",
            " - 0s - loss: 31.6313 - mean_absolute_error: 4.3078 - val_loss: 38.1801 - val_mean_absolute_error: 4.6831\n",
            "Epoch 16/20\n",
            " - 0s - loss: 29.0940 - mean_absolute_error: 4.0815 - val_loss: 35.5369 - val_mean_absolute_error: 4.4091\n",
            "Epoch 17/20\n",
            " - 0s - loss: 27.8671 - mean_absolute_error: 3.9753 - val_loss: 33.9585 - val_mean_absolute_error: 4.2029\n",
            "Epoch 18/20\n",
            " - 0s - loss: 24.6189 - mean_absolute_error: 3.6550 - val_loss: 32.4576 - val_mean_absolute_error: 4.1337\n",
            "Epoch 19/20\n",
            " - 0s - loss: 25.2965 - mean_absolute_error: 3.7654 - val_loss: 31.1475 - val_mean_absolute_error: 4.0576\n",
            "Epoch 20/20\n",
            " - 0s - loss: 22.7383 - mean_absolute_error: 3.5613 - val_loss: 30.0796 - val_mean_absolute_error: 3.9165\n",
            "Train MAE:  3.3519 , Train Loss:  21.1302\n",
            "Val MAE:  3.9165 , Val Loss:  30.0796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgPpzX-MJ0O4",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64YusphI6y2h",
        "colab_type": "code",
        "outputId": "f4803796-880c-437e-d867-57973beff7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "import keras\n",
        "import keras.utils\n",
        "from keras import utils as np_utils\n",
        "from sklearn import preprocessing\n",
        "df = pd.read_csv('Heart.csv')\n",
        "x = df[df.columns[:12]]\n",
        "y = df.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.7, random_state =  90)\n",
        "#Select numerical columns which needs to be normalized\n",
        "train_norm = x_train[['age','trestbps','chol','thalach']]\n",
        "test_norm = x_test[['age','trestbps','chol','thalach']]\n",
        "#.loc(['age','trestbps','chol','thalach'])\n",
        "# Normalize Training Data\n",
        "std_scale = preprocessing.StandardScaler().fit(train_norm)\n",
        "x_train_norm = std_scale.transform(train_norm)\n",
        "#Converting numpy array to dataframe\n",
        "training_norm_col = pd.DataFrame(x_train_norm, index=train_norm.index, columns=train_norm.columns)\n",
        "x_train.update(training_norm_col)\n",
        "print (x_train.head())\n",
        "# Normalize Testing Data by using mean and SD of training set\n",
        "x_test_norm = std_scale.transform(test_norm)\n",
        "testing_norm_col = pd.DataFrame(x_test_norm, index=test_norm.index, columns=test_norm.columns)\n",
        "x_test.update(testing_norm_col)\n",
        "print (x_train.head())\n",
        "\n",
        "from tensorboardcolab import *\n",
        "tbc=TensorBoardColab()\n",
        "#Build neural network model with normalized data\n",
        "model = keras.Sequential([\n",
        " keras.layers.Dense(64, activation=tf.nn.relu,\n",
        " input_shape=(x_train.shape[1],)),\n",
        " keras.layers.Dense(64, activation=tf.nn.relu),\n",
        " keras.layers.Dense(8, activation=  'softmax')\n",
        " ])\n",
        "\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history2 = model.fit(\n",
        " x_train, y_train,\n",
        " epochs= 26, batch_size = 60,\n",
        " validation_data = (x_test, y_test),callbacks=[TensorBoardColabCallback(tbc)])\n",
        "score=model.evaluate(x_test,y_test)\n",
        "print('test accuracy',score[1])\n",
        "\n",
        "# changing hyperparameters\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history2 = model.fit(\n",
        " x_train, y_train,\n",
        " epochs= 50, batch_size = 128,\n",
        " validation_data = (x_test, y_test),callbacks=[TensorBoardColabCallback(tbc)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:5819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[col] = expressions.where(mask, this, that)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:5819: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[col] = expressions.where(mask, this, that)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "          age  sex  cp  trestbps      chol  ...   thalach  exang  oldpeak  slope  ca\n",
            "239 -2.049567    1   0 -0.328644  0.804527  ...  0.282575      1      0.0      2   0\n",
            "60   1.852227    0   2 -1.278995  0.438565  ... -0.799812      0      0.0      2   1\n",
            "270 -0.857352    1   0 -0.685026  0.094131  ... -0.216988      0      0.8      2   0\n",
            "57  -0.965735    1   0 -0.982011  0.330929  ...  1.489854      0      0.0      2   0\n",
            "278  0.443246    0   1  0.265325  1.601032  ...  0.116054      0      0.0      2   2\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "          age  sex  cp  trestbps      chol  ...   thalach  exang  oldpeak  slope  ca\n",
            "239 -2.049567    1   0 -0.328644  0.804527  ...  0.282575      1      0.0      2   0\n",
            "60   1.852227    0   2 -1.278995  0.438565  ... -0.799812      0      0.0      2   1\n",
            "270 -0.857352    1   0 -0.685026  0.094131  ... -0.216988      0      0.8      2   0\n",
            "57  -0.965735    1   0 -0.982011  0.330929  ...  1.489854      0      0.0      2   0\n",
            "278  0.443246    0   1  0.265325  1.601032  ...  0.116054      0      0.0      2   2\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://498de53e.ngrok.io\n",
            "Train on 212 samples, validate on 91 samples\n",
            "Epoch 1/26\n",
            "212/212 [==============================] - 0s 852us/step - loss: 1.6979 - acc: 0.4481 - val_loss: 1.4724 - val_acc: 0.4945\n",
            "Epoch 2/26\n",
            "212/212 [==============================] - 0s 57us/step - loss: 1.4108 - acc: 0.5991 - val_loss: 1.2288 - val_acc: 0.6264\n",
            "Epoch 3/26\n",
            "212/212 [==============================] - 0s 85us/step - loss: 1.1646 - acc: 0.6840 - val_loss: 1.0315 - val_acc: 0.6813\n",
            "Epoch 4/26\n",
            "212/212 [==============================] - 0s 61us/step - loss: 0.9628 - acc: 0.7594 - val_loss: 0.8783 - val_acc: 0.7143\n",
            "Epoch 5/26\n",
            "212/212 [==============================] - 0s 71us/step - loss: 0.7973 - acc: 0.8255 - val_loss: 0.7639 - val_acc: 0.7143\n",
            "Epoch 6/26\n",
            "212/212 [==============================] - 0s 68us/step - loss: 0.6708 - acc: 0.8255 - val_loss: 0.6828 - val_acc: 0.6923\n",
            "Epoch 7/26\n",
            "212/212 [==============================] - 0s 60us/step - loss: 0.5796 - acc: 0.8302 - val_loss: 0.6258 - val_acc: 0.6923\n",
            "Epoch 8/26\n",
            "212/212 [==============================] - 0s 61us/step - loss: 0.5144 - acc: 0.8255 - val_loss: 0.5885 - val_acc: 0.7033\n",
            "Epoch 9/26\n",
            "212/212 [==============================] - 0s 66us/step - loss: 0.4670 - acc: 0.8255 - val_loss: 0.5645 - val_acc: 0.7143\n",
            "Epoch 10/26\n",
            "212/212 [==============================] - 0s 72us/step - loss: 0.4366 - acc: 0.8302 - val_loss: 0.5503 - val_acc: 0.7143\n",
            "Epoch 11/26\n",
            "212/212 [==============================] - 0s 63us/step - loss: 0.4130 - acc: 0.8349 - val_loss: 0.5405 - val_acc: 0.7143\n",
            "Epoch 12/26\n",
            "212/212 [==============================] - 0s 57us/step - loss: 0.3977 - acc: 0.8396 - val_loss: 0.5341 - val_acc: 0.7363\n",
            "Epoch 13/26\n",
            "212/212 [==============================] - 0s 58us/step - loss: 0.3857 - acc: 0.8443 - val_loss: 0.5310 - val_acc: 0.7253\n",
            "Epoch 14/26\n",
            "212/212 [==============================] - 0s 60us/step - loss: 0.3746 - acc: 0.8491 - val_loss: 0.5259 - val_acc: 0.7473\n",
            "Epoch 15/26\n",
            "212/212 [==============================] - 0s 76us/step - loss: 0.3660 - acc: 0.8538 - val_loss: 0.5234 - val_acc: 0.7473\n",
            "Epoch 16/26\n",
            "212/212 [==============================] - 0s 70us/step - loss: 0.3613 - acc: 0.8443 - val_loss: 0.5199 - val_acc: 0.7473\n",
            "Epoch 17/26\n",
            "212/212 [==============================] - 0s 83us/step - loss: 0.3535 - acc: 0.8538 - val_loss: 0.5179 - val_acc: 0.7582\n",
            "Epoch 18/26\n",
            "212/212 [==============================] - 0s 69us/step - loss: 0.3477 - acc: 0.8632 - val_loss: 0.5180 - val_acc: 0.7473\n",
            "Epoch 19/26\n",
            "212/212 [==============================] - 0s 60us/step - loss: 0.3438 - acc: 0.8679 - val_loss: 0.5190 - val_acc: 0.7363\n",
            "Epoch 20/26\n",
            "212/212 [==============================] - 0s 62us/step - loss: 0.3389 - acc: 0.8679 - val_loss: 0.5149 - val_acc: 0.7582\n",
            "Epoch 21/26\n",
            "212/212 [==============================] - 0s 60us/step - loss: 0.3341 - acc: 0.8632 - val_loss: 0.5114 - val_acc: 0.7692\n",
            "Epoch 22/26\n",
            "212/212 [==============================] - 0s 58us/step - loss: 0.3309 - acc: 0.8679 - val_loss: 0.5112 - val_acc: 0.7692\n",
            "Epoch 23/26\n",
            "212/212 [==============================] - 0s 60us/step - loss: 0.3269 - acc: 0.8679 - val_loss: 0.5102 - val_acc: 0.7802\n",
            "Epoch 24/26\n",
            "212/212 [==============================] - 0s 84us/step - loss: 0.3238 - acc: 0.8679 - val_loss: 0.5093 - val_acc: 0.7802\n",
            "Epoch 25/26\n",
            "212/212 [==============================] - 0s 62us/step - loss: 0.3210 - acc: 0.8774 - val_loss: 0.5066 - val_acc: 0.7802\n",
            "Epoch 26/26\n",
            "212/212 [==============================] - 0s 65us/step - loss: 0.3167 - acc: 0.8726 - val_loss: 0.5041 - val_acc: 0.7692\n",
            "91/91 [==============================] - 0s 80us/step\n",
            "test accuracy 0.7692307666107848\n",
            "Train on 212 samples, validate on 91 samples\n",
            "Epoch 1/50\n",
            "212/212 [==============================] - 0s 863us/step - loss: 0.3156 - acc: 0.8774 - val_loss: 0.5054 - val_acc: 0.7802\n",
            "Epoch 2/50\n",
            "212/212 [==============================] - 0s 40us/step - loss: 0.3146 - acc: 0.8726 - val_loss: 0.5071 - val_acc: 0.7912\n",
            "Epoch 3/50\n",
            "212/212 [==============================] - 0s 41us/step - loss: 0.3136 - acc: 0.8774 - val_loss: 0.5071 - val_acc: 0.7912\n",
            "Epoch 4/50\n",
            "212/212 [==============================] - 0s 38us/step - loss: 0.3127 - acc: 0.8774 - val_loss: 0.5053 - val_acc: 0.7912\n",
            "Epoch 5/50\n",
            "212/212 [==============================] - 0s 49us/step - loss: 0.3108 - acc: 0.8774 - val_loss: 0.5034 - val_acc: 0.7912\n",
            "Epoch 6/50\n",
            "212/212 [==============================] - 0s 36us/step - loss: 0.3103 - acc: 0.8726 - val_loss: 0.5025 - val_acc: 0.7692\n",
            "Epoch 7/50\n",
            "212/212 [==============================] - 0s 38us/step - loss: 0.3091 - acc: 0.8774 - val_loss: 0.5024 - val_acc: 0.7802\n",
            "Epoch 8/50\n",
            "212/212 [==============================] - 0s 38us/step - loss: 0.3081 - acc: 0.8774 - val_loss: 0.5037 - val_acc: 0.7912\n",
            "Epoch 9/50\n",
            "212/212 [==============================] - 0s 34us/step - loss: 0.3061 - acc: 0.8726 - val_loss: 0.5049 - val_acc: 0.7912\n",
            "Epoch 10/50\n",
            "212/212 [==============================] - 0s 52us/step - loss: 0.3048 - acc: 0.8774 - val_loss: 0.5046 - val_acc: 0.7912\n",
            "Epoch 11/50\n",
            "212/212 [==============================] - 0s 34us/step - loss: 0.3053 - acc: 0.8774 - val_loss: 0.5030 - val_acc: 0.7802\n",
            "Epoch 12/50\n",
            "212/212 [==============================] - 0s 37us/step - loss: 0.3023 - acc: 0.8774 - val_loss: 0.5020 - val_acc: 0.7912\n",
            "Epoch 13/50\n",
            "212/212 [==============================] - 0s 34us/step - loss: 0.3009 - acc: 0.8774 - val_loss: 0.5017 - val_acc: 0.7802\n",
            "Epoch 14/50\n",
            "212/212 [==============================] - 0s 34us/step - loss: 0.2989 - acc: 0.8774 - val_loss: 0.5021 - val_acc: 0.7912\n",
            "Epoch 15/50\n",
            "212/212 [==============================] - 0s 35us/step - loss: 0.2980 - acc: 0.8774 - val_loss: 0.5015 - val_acc: 0.7912\n",
            "Epoch 16/50\n",
            "212/212 [==============================] - 0s 38us/step - loss: 0.2971 - acc: 0.8821 - val_loss: 0.4990 - val_acc: 0.7912\n",
            "Epoch 17/50\n",
            "212/212 [==============================] - 0s 35us/step - loss: 0.2964 - acc: 0.8821 - val_loss: 0.4961 - val_acc: 0.7912\n",
            "Epoch 18/50\n",
            "212/212 [==============================] - 0s 35us/step - loss: 0.2952 - acc: 0.8915 - val_loss: 0.4949 - val_acc: 0.7912\n",
            "Epoch 19/50\n",
            "212/212 [==============================] - 0s 31us/step - loss: 0.2932 - acc: 0.8821 - val_loss: 0.4959 - val_acc: 0.8022\n",
            "Epoch 20/50\n",
            "212/212 [==============================] - 0s 34us/step - loss: 0.2905 - acc: 0.8868 - val_loss: 0.4970 - val_acc: 0.8022\n",
            "Epoch 21/50\n",
            "212/212 [==============================] - 0s 41us/step - loss: 0.2917 - acc: 0.8868 - val_loss: 0.4945 - val_acc: 0.8022\n",
            "Epoch 22/50\n",
            "212/212 [==============================] - 0s 43us/step - loss: 0.2879 - acc: 0.8868 - val_loss: 0.4911 - val_acc: 0.8022\n",
            "Epoch 23/50\n",
            "212/212 [==============================] - 0s 42us/step - loss: 0.2875 - acc: 0.8868 - val_loss: 0.4897 - val_acc: 0.8022\n",
            "Epoch 24/50\n",
            "212/212 [==============================] - 0s 41us/step - loss: 0.2853 - acc: 0.8868 - val_loss: 0.4897 - val_acc: 0.8132\n",
            "Epoch 25/50\n",
            "212/212 [==============================] - 0s 42us/step - loss: 0.2843 - acc: 0.8821 - val_loss: 0.4907 - val_acc: 0.8132\n",
            "Epoch 26/50\n",
            "212/212 [==============================] - 0s 45us/step - loss: 0.2858 - acc: 0.8821 - val_loss: 0.4887 - val_acc: 0.8132\n",
            "Epoch 27/50\n",
            "212/212 [==============================] - 0s 40us/step - loss: 0.2815 - acc: 0.8915 - val_loss: 0.4862 - val_acc: 0.8132\n",
            "Epoch 28/50\n",
            "212/212 [==============================] - 0s 40us/step - loss: 0.2806 - acc: 0.8915 - val_loss: 0.4861 - val_acc: 0.8132\n",
            "Epoch 29/50\n",
            "212/212 [==============================] - 0s 33us/step - loss: 0.2791 - acc: 0.8868 - val_loss: 0.4857 - val_acc: 0.8132\n",
            "Epoch 30/50\n",
            "212/212 [==============================] - 0s 43us/step - loss: 0.2789 - acc: 0.8868 - val_loss: 0.4831 - val_acc: 0.8242\n",
            "Epoch 31/50\n",
            "212/212 [==============================] - 0s 50us/step - loss: 0.2761 - acc: 0.8915 - val_loss: 0.4808 - val_acc: 0.8242\n",
            "Epoch 32/50\n",
            "212/212 [==============================] - 0s 49us/step - loss: 0.2753 - acc: 0.8962 - val_loss: 0.4798 - val_acc: 0.8242\n",
            "Epoch 33/50\n",
            "212/212 [==============================] - 0s 58us/step - loss: 0.2739 - acc: 0.9009 - val_loss: 0.4798 - val_acc: 0.8242\n",
            "Epoch 34/50\n",
            "212/212 [==============================] - 0s 42us/step - loss: 0.2724 - acc: 0.9009 - val_loss: 0.4812 - val_acc: 0.8242\n",
            "Epoch 35/50\n",
            "212/212 [==============================] - 0s 52us/step - loss: 0.2712 - acc: 0.8868 - val_loss: 0.4815 - val_acc: 0.8132\n",
            "Epoch 36/50\n",
            "212/212 [==============================] - 0s 45us/step - loss: 0.2701 - acc: 0.8868 - val_loss: 0.4794 - val_acc: 0.8242\n",
            "Epoch 37/50\n",
            "212/212 [==============================] - 0s 53us/step - loss: 0.2695 - acc: 0.9009 - val_loss: 0.4784 - val_acc: 0.8242\n",
            "Epoch 38/50\n",
            "212/212 [==============================] - 0s 59us/step - loss: 0.2674 - acc: 0.9009 - val_loss: 0.4784 - val_acc: 0.8242\n",
            "Epoch 39/50\n",
            "212/212 [==============================] - 0s 47us/step - loss: 0.2664 - acc: 0.8962 - val_loss: 0.4781 - val_acc: 0.8242\n",
            "Epoch 40/50\n",
            "212/212 [==============================] - 0s 54us/step - loss: 0.2650 - acc: 0.9009 - val_loss: 0.4772 - val_acc: 0.8242\n",
            "Epoch 41/50\n",
            "212/212 [==============================] - 0s 37us/step - loss: 0.2645 - acc: 0.9009 - val_loss: 0.4777 - val_acc: 0.8242\n",
            "Epoch 42/50\n",
            "212/212 [==============================] - 0s 37us/step - loss: 0.2629 - acc: 0.9009 - val_loss: 0.4786 - val_acc: 0.8242\n",
            "Epoch 43/50\n",
            "212/212 [==============================] - 0s 39us/step - loss: 0.2620 - acc: 0.8962 - val_loss: 0.4784 - val_acc: 0.8242\n",
            "Epoch 44/50\n",
            "212/212 [==============================] - 0s 36us/step - loss: 0.2605 - acc: 0.8962 - val_loss: 0.4782 - val_acc: 0.8242\n",
            "Epoch 45/50\n",
            "212/212 [==============================] - 0s 49us/step - loss: 0.2597 - acc: 0.8962 - val_loss: 0.4785 - val_acc: 0.8242\n",
            "Epoch 46/50\n",
            "212/212 [==============================] - 0s 40us/step - loss: 0.2588 - acc: 0.9009 - val_loss: 0.4788 - val_acc: 0.8242\n",
            "Epoch 47/50\n",
            "212/212 [==============================] - 0s 37us/step - loss: 0.2586 - acc: 0.9057 - val_loss: 0.4804 - val_acc: 0.8242\n",
            "Epoch 48/50\n",
            "212/212 [==============================] - 0s 50us/step - loss: 0.2564 - acc: 0.9009 - val_loss: 0.4826 - val_acc: 0.8242\n",
            "Epoch 49/50\n",
            "212/212 [==============================] - 0s 58us/step - loss: 0.2553 - acc: 0.8962 - val_loss: 0.4832 - val_acc: 0.8242\n",
            "Epoch 50/50\n",
            "212/212 [==============================] - 0s 37us/step - loss: 0.2546 - acc: 0.9009 - val_loss: 0.4829 - val_acc: 0.8242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEzTlsPJAOyB",
        "colab_type": "code",
        "outputId": "87d8cc0b-3a9a-4946-c09f-9c914c35eea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        (os.path.join(dirname, filename))\n",
        "# Any results you write to the current directory are saved as output.\n",
        "labels = os.listdir('natural-images/natural_images/')\n",
        "print(labels)\n",
        "from IPython.display import Image, display\n",
        "num = []\n",
        "for label in labels:\n",
        "    path = 'natural-images/natural_images/{0}/'.format(label)\n",
        "    folder_data = os.listdir(path)\n",
        "    k = 0\n",
        "    print('\\n', label.upper())\n",
        "    for image_path in folder_data:\n",
        "        if k < 5:\n",
        "            display(Image(path+image_path))\n",
        "        k = k+1\n",
        "    num.append(k)\n",
        "    print('there are ', k,' images in ', label, 'class')\n",
        "\n",
        "\n",
        "x_data =[]\n",
        "y_data = []\n",
        "import cv2\n",
        "for label in labels:\n",
        "    path = 'natural-images/natural_images/{0}/'.format(label)\n",
        "    folder_data = os.listdir(path)\n",
        "    for image_path in folder_data:\n",
        "        image = cv2.imread(path+image_path)\n",
        "        image_resized = cv2.resize(image, (32,32))\n",
        "        x_data.append(np.array(image_resized))\n",
        "        y_data.append(label)\n",
        "\n",
        "x_data = np.array(x_data)\n",
        "y_data = np.array(y_data)\n",
        "print('the shape of X is: ', x_data.shape, 'and that of Y is: ', y_data.shape)\n",
        "#stadardizing the input data\n",
        "x_data = x_data.astype('float32')/255\n",
        "\n",
        "#converting the y_data into categorical:\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "y_encoded = LabelEncoder().fit_transform(y_data)\n",
        "\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "#lets shuffle all the data we have:\n",
        "r = np.arange(x_data.shape[0])\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(r)\n",
        "X = x_data[r]\n",
        "Y = y_categorical[r]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n",
        "\n",
        "#structuring the CNN model\n",
        "from keras import models, layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(layers.Dropout(rate=0.25))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(rate=0.5))\n",
        "model.add(layers.Dense(8, activation='softmax'))\n",
        "\n",
        "#let's compile the model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "#fitting the model\n",
        "history = model.fit(X_train, Y_train, epochs=25, validation_split=0.2)\n",
        "\n",
        "#Display of the accuracy and the loss values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (8,8))\n",
        "plt.plot(history.history['accuracy'], label='training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
        "plt.plot(history.history['loss'], label='training loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.title('Loss/accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "Y_pred = model.predict_classes(X_test)\n",
        "#converting over Y test to actual labels.\n",
        "Y_test = np.argmax(Y_test, axis = 1)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('the accuracy obtained on the test set is:', accuracy_score(Y_pred,Y_test))\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4205e4ef92cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Any results you write to the current directory are saved as output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'natural-images/natural_images/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'natural-images/natural_images/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyKTn61lXz32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}